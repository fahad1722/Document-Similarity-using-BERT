<!DOCTYPE html>
<html>
  <head>
    <title>Document Similarity - Docs</title>
    <style>
      body {
        font-family: Arial, sans-serif;
        background-color: #f2f2f2;
        margin: 0;
        padding: 0;
      }

      .navbar {
        background-color: #333;
        overflow: hidden;
      }

      .navbar a {
        float: left;
        width: 120px;
        color: #f2f2f2;
        text-align: center;
        padding: 14px;
        text-decoration: none;
        font-size: 17px;
        flex: 1;
      }

      .navbar a:hover {
        background-color: #ddd;
        color: black;
      }

      .navbar a.active {
        background-color: #004080;
        color: white;
      }
      .container {
        margin: 30px;
        padding: 20px;
        background-color: #fff;
        border-radius: 5px;
        box-shadow: 0 2px 5px rgba(0, 0, 0, 0.2);
      }

      .right-heading {
        margin-bottom: 20px;
        font-size: 24px;
        color: #333;
      }

      p {
        color: #666;
        text-align: justify;
      }

      ul {
        margin-top: 10px;
        padding-left: 20px;
      }

      ul li {
        margin-bottom: 5px;
      }
    </style>
  </head>
  <body>
    <!-- Navigation bar -->
    <div class="navbar">
      <div class="navbar-links">
        <a href="upload.html">Home</a>
        <a class="active" href="#">Docs</a>
      </div>
    </div>

    <!-- Main container -->
    <div class="container">
      <div class="right-heading">
        <h1>BERT: Advantages over TF-IDF</h1>
      </div>
      <p>
        BERT (Bidirectional Encoder Representations from Transformers) is a
        pre-trained natural language processing model that has several
        advantages over traditional methods like TF-IDF (Term Frequency-Inverse
        Document Frequency). Some of the key advantages are:
      </p>
      <ul>
        <li>
          BERT can capture contextual information and understand the nuances of
          language.
        </li>
        <li>It provides more accurate representations of word meanings.</li>
        <li>
          BERT excels in handling sentence-level and document-level tasks.
        </li>
        <li>It can handle out-of-vocabulary words effectively.</li>
        <li>BERT can handle tasks involving ambiguity and polysemy better.</li>
      </ul>
      <p>
        These advantages make BERT a powerful tool for various natural language
        processing tasks, including document similarity, text classification,
        and question answering.
      </p>
      <h2>Word Embeddings</h2>
      <p>
        Word embeddings are dense vector representations of words that capture
        semantic and syntactic information. BERT's pre-training process involves
        learning powerful word embeddings, which enable it to understand the
        meaning and relationships between words.
      </p>
      <h2>Cosine Similarity</h2>
      <p>
        Cosine similarity is a metric used to measure the similarity between two
        vectors. In the context of document similarity, BERT calculates the
        cosine similarity between the embeddings of two documents to determine
        their similarity. Higher cosine similarity scores indicate greater
        similarity between the documents.
      </p>
      <h2>Visualization</h2>
      <p>
        BERT embeddings can be visualized using techniques like t-SNE
        (t-Distributed Stochastic Neighbor Embedding) to project
        high-dimensional embeddings into a lower-dimensional space. This
        visualization can provide insights into the clustering and relationships
        between different documents.
      </p>
    </div>
  </body>
</html>
